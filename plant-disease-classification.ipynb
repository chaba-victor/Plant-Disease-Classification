{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  PLANT DISEASE CLASSIFICATION USING RESNET-9 ","metadata":{}},{"cell_type":"markdown","source":"# Dataset description","metadata":{}},{"cell_type":"markdown","source":"This dataset has been generated by applying offline augmentation techniques to the original dataset. You can find the original PlantVillage Dataset [here](https://github.com/spMohanty/PlantVillage-Dataset).The dataset comprises approximately 87,000 RGB images of crop leaves, both healthy and diseased, categorized into 38 different classes. The dataset is divided into a training set and a validation set, maintaining an 80/20 ratio, while preserving the original directory structure. Additionally, a separate directory is created for prediction purposes, containing 33 test images.\n\nIt is important to note that this description is provided within the dataset itself.","metadata":{}},{"cell_type":"markdown","source":"# The goal \nTo build a model, which can classify between healthy and diseased crop leaves and also if the crop have any disease, predict which disease is it.","metadata":{}},{"cell_type":"markdown","source":"## Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:29:47.770724Z","iopub.execute_input":"2022-11-22T10:29:47.771021Z","iopub.status.idle":"2022-11-22T10:29:56.482313Z","shell.execute_reply.started":"2022-11-22T10:29:47.770983Z","shell.execute_reply":"2022-11-22T10:29:56.481215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pytorch natively doesn't print a nicely formatted and pretty summary of the model like in keras so we'll require torchsummary library for that.","metadata":{}},{"cell_type":"code","source":"import os                       # for working with files\nimport numpy as np              # for numerical computationss\nimport pandas as pd             # for working with dataframes\nimport torch                    # Pytorch module \nimport matplotlib.pyplot as plt # for plotting informations on graph and images using tensors\nimport torch.nn as nn           # for creating  neural networks\nfrom torch.utils.data import DataLoader # for dataloaders \nfrom PIL import Image           # for checking images\nimport torch.nn.functional as F # for functions for calculating loss\nimport torchvision.transforms as transforms   # for transforming images into tensors \nfrom torchvision.utils import make_grid       # for data checking\nfrom torchvision.datasets import ImageFolder  # for working with classes and images\nfrom torchsummary import summary              # for getting the summary of our model\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-22T10:29:56.484514Z","iopub.execute_input":"2022-11-22T10:29:56.484873Z","iopub.status.idle":"2022-11-22T10:29:58.183568Z","shell.execute_reply.started":"2022-11-22T10:29:56.484829Z","shell.execute_reply":"2022-11-22T10:29:58.182669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Data exploration","metadata":{}},{"cell_type":"markdown","source":"Loading the data ","metadata":{}},{"cell_type":"code","source":"data_dir = \"../input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)\"\ntrain_dir = data_dir + \"/train\"\nvalid_dir = data_dir + \"/valid\"\ndiseases = os.listdir(train_dir)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:29:58.184936Z","iopub.execute_input":"2022-11-22T10:29:58.185292Z","iopub.status.idle":"2022-11-22T10:29:58.212035Z","shell.execute_reply.started":"2022-11-22T10:29:58.185253Z","shell.execute_reply":"2022-11-22T10:29:58.211425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing the disease names\nprint(diseases)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:29:58.213750Z","iopub.execute_input":"2022-11-22T10:29:58.213987Z","iopub.status.idle":"2022-11-22T10:29:58.220211Z","shell.execute_reply.started":"2022-11-22T10:29:58.213963Z","shell.execute_reply":"2022-11-22T10:29:58.219178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total disease classes are: {}\".format(len(diseases)))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:29:58.223952Z","iopub.execute_input":"2022-11-22T10:29:58.224525Z","iopub.status.idle":"2022-11-22T10:29:58.229958Z","shell.execute_reply.started":"2022-11-22T10:29:58.224487Z","shell.execute_reply":"2022-11-22T10:29:58.229119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plants = []\nNumberOfDiseases = 0\nfor plant in diseases:\n    if plant.split('___')[0] not in plants:\n        plants.append(plant.split('___')[0])\n    if plant.split('___')[1] != 'healthy':\n        NumberOfDiseases += 1","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:29:58.233451Z","iopub.execute_input":"2022-11-22T10:29:58.233834Z","iopub.status.idle":"2022-11-22T10:29:58.239954Z","shell.execute_reply.started":"2022-11-22T10:29:58.233798Z","shell.execute_reply":"2022-11-22T10:29:58.239003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above cell extract the number of unique plants and number of unique diseases","metadata":{}},{"cell_type":"code","source":"# unique plants in the dataset\nprint(f\"Unique Plants are: \\n{plants}\")","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:29:58.241217Z","iopub.execute_input":"2022-11-22T10:29:58.241744Z","iopub.status.idle":"2022-11-22T10:29:58.251333Z","shell.execute_reply.started":"2022-11-22T10:29:58.241697Z","shell.execute_reply":"2022-11-22T10:29:58.250128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of unique plants\nprint(\"Number of plants: {}\".format(len(plants)))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:29:58.252565Z","iopub.execute_input":"2022-11-22T10:29:58.253080Z","iopub.status.idle":"2022-11-22T10:29:58.262490Z","shell.execute_reply.started":"2022-11-22T10:29:58.253043Z","shell.execute_reply":"2022-11-22T10:29:58.261624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of unique diseases\nprint(\"Number of diseases: {}\".format(NumberOfDiseases))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:29:58.263747Z","iopub.execute_input":"2022-11-22T10:29:58.264322Z","iopub.status.idle":"2022-11-22T10:29:58.272643Z","shell.execute_reply.started":"2022-11-22T10:29:58.264286Z","shell.execute_reply":"2022-11-22T10:29:58.271855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of images for each disease\nnums = {}\nfor disease in diseases:\n    nums[disease] = len(os.listdir(train_dir + '/' + disease))\n    \n# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\n\nimg_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=[\"no. of images\"])\nimg_per_class","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:29:58.274061Z","iopub.execute_input":"2022-11-22T10:29:58.274676Z","iopub.status.idle":"2022-11-22T10:30:12.385970Z","shell.execute_reply.started":"2022-11-22T10:29:58.274638Z","shell.execute_reply":"2022-11-22T10:30:12.385262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualizing the above information on a graph","metadata":{}},{"cell_type":"code","source":"# plotting number of images available for each disease\nindex = [n for n in range(38)]\nplt.figure(figsize=(20, 5))\nplt.bar(index, [n for n in nums.values()], width=0.3)\nplt.xlabel('Plants/Diseases', fontsize=10)\nplt.ylabel('No of images available', fontsize=10)\nplt.xticks(index, diseases, fontsize=5, rotation=90)\nplt.title('Images per each class of plant disease')","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:30:12.387370Z","iopub.execute_input":"2022-11-22T10:30:12.387707Z","iopub.status.idle":"2022-11-22T10:30:12.739430Z","shell.execute_reply.started":"2022-11-22T10:30:12.387671Z","shell.execute_reply":"2022-11-22T10:30:12.738523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the dataset is almost balanced for all classes, so we are good to go forward","metadata":{}},{"cell_type":"markdown","source":"#### Images available for training","metadata":{}},{"cell_type":"code","source":"n_train = 0\nfor value in nums.values():\n    n_train += value\nprint(f\"There are {n_train} images for training\")","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:30:12.740855Z","iopub.execute_input":"2022-11-22T10:30:12.741423Z","iopub.status.idle":"2022-11-22T10:30:12.748220Z","shell.execute_reply.started":"2022-11-22T10:30:12.741377Z","shell.execute_reply":"2022-11-22T10:30:12.747289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparation of data for training ","metadata":{}},{"cell_type":"code","source":"# datasets for validation and training\ntrain = ImageFolder(train_dir, transform=transforms.ToTensor())\nvalid = ImageFolder(valid_dir, transform=transforms.ToTensor()) ","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:30:12.749700Z","iopub.execute_input":"2022-11-22T10:30:12.750292Z","iopub.status.idle":"2022-11-22T10:31:58.869810Z","shell.execute_reply.started":"2022-11-22T10:30:12.750239Z","shell.execute_reply":"2022-11-22T10:31:58.868827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`torchvision.datasets` is a class which helps in loading all common and famous datasets. It also helps in loading custom datasets. I have used subclass `torchvision.datasets.ImageFolder` which helps in loading the image data when the data is arranged in this way:\n\n----------------\nroot/dog/xxx.png\n\nroot/dog/xxy.png\n\nroot/dog/xxz.png\n\n<br>\n\nroot/cat/123.png\n\nroot/cat/nsdf3.png\n\nroot/cat/asd932_.png\n\n----------------","metadata":{}},{"cell_type":"markdown","source":"Next, after loading the data, we need to transform the pixel values of each image (0-255) to 0-1 as neural networks works well with normalized data. The entire array of pixel values is converted to torch [tensor](https://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_tensor.html#:~:text=A%20PyTorch%20Tensor%20is%20basically,used%20for%20arbitrary%20numeric%20computation.) and then divided by 255.\nIf you are not familiar why normalizing inputs help neural network, read [this](https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d) post.","metadata":{}},{"cell_type":"markdown","source":"#### Image shape ","metadata":{}},{"cell_type":"code","source":"img, label = train[0]\nprint(img.shape, label)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:31:58.871143Z","iopub.execute_input":"2022-11-22T10:31:58.871605Z","iopub.status.idle":"2022-11-22T10:31:58.954499Z","shell.execute_reply.started":"2022-11-22T10:31:58.871569Z","shell.execute_reply":"2022-11-22T10:31:58.953669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the shape (3, 256 256) of the image. 3 is the number of channels (RGB) and 256 x 256 is the width and height of the image","metadata":{}},{"cell_type":"code","source":"# total number of classes in train set\nlen(train.classes)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:31:58.957376Z","iopub.execute_input":"2022-11-22T10:31:58.960166Z","iopub.status.idle":"2022-11-22T10:31:58.967575Z","shell.execute_reply.started":"2022-11-22T10:31:58.960125Z","shell.execute_reply":"2022-11-22T10:31:58.966753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for checking some images from training dataset\ndef show_image(image, label):\n    print(\"Label :\" + train.classes[label] + \"(\" + str(label) + \")\")\n    plt.imshow(image.permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:31:58.969114Z","iopub.execute_input":"2022-11-22T10:31:58.970576Z","iopub.status.idle":"2022-11-22T10:31:58.982149Z","shell.execute_reply.started":"2022-11-22T10:31:58.970537Z","shell.execute_reply":"2022-11-22T10:31:58.981339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Some Images from training dataset","metadata":{}},{"cell_type":"code","source":"show_image(*train[0])","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:31:58.983523Z","iopub.execute_input":"2022-11-22T10:31:58.984236Z","iopub.status.idle":"2022-11-22T10:31:59.212830Z","shell.execute_reply.started":"2022-11-22T10:31:58.984198Z","shell.execute_reply":"2022-11-22T10:31:59.211905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image(*train[70000])","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:31:59.213988Z","iopub.execute_input":"2022-11-22T10:31:59.214494Z","iopub.status.idle":"2022-11-22T10:31:59.449238Z","shell.execute_reply.started":"2022-11-22T10:31:59.214452Z","shell.execute_reply":"2022-11-22T10:31:59.448380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image(*train[30000])","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:31:59.450870Z","iopub.execute_input":"2022-11-22T10:31:59.451227Z","iopub.status.idle":"2022-11-22T10:31:59.601696Z","shell.execute_reply.started":"2022-11-22T10:31:59.451190Z","shell.execute_reply":"2022-11-22T10:31:59.600885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the seed value\nrandom_seed = 7\ntorch.manual_seed(random_seed)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:31:59.603061Z","iopub.execute_input":"2022-11-22T10:31:59.603649Z","iopub.status.idle":"2022-11-22T10:31:59.612861Z","shell.execute_reply.started":"2022-11-22T10:31:59.603609Z","shell.execute_reply":"2022-11-22T10:31:59.612091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting the batch size\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:31:59.614405Z","iopub.execute_input":"2022-11-22T10:31:59.614752Z","iopub.status.idle":"2022-11-22T10:31:59.619750Z","shell.execute_reply.started":"2022-11-22T10:31:59.614715Z","shell.execute_reply":"2022-11-22T10:31:59.618346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`batch_size` is the total number of images given as input at once in forward propagation of the CNN. In essence, the batch size determines the number of samples that are processed together during network training.\n\nTo provide an example, let's consider a scenario where you have a training dataset consisting of 1050 samples and you decide to set the batch size to 100. The algorithm will take the first 100 samples (from the 1st to the 100th) from the training dataset and train the network using these samples. Then, it will move on to the next 100 samples (from the 101st to the 200th) and repeat the training process. This procedure will continue until all samples have been processed through the network.","metadata":{}},{"cell_type":"code","source":"# DataLoaders for training and validation\ntrain_dl = DataLoader(train, batch_size, shuffle=True, num_workers=2, pin_memory=True)\nvalid_dl = DataLoader(valid, batch_size, num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:31:59.620973Z","iopub.execute_input":"2022-11-22T10:31:59.621254Z","iopub.status.idle":"2022-11-22T10:31:59.629710Z","shell.execute_reply.started":"2022-11-22T10:31:59.621227Z","shell.execute_reply":"2022-11-22T10:31:59.628754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `DataLoader` is a subclass which comes from `torch.utils.data`. It helps in loading large and memory consuming datasets. It takes in `batch_size` which denotes the number of samples contained in each generated batch. \n\n- Setting `shuffle=True` shuffles the dataset. It is heplful so that batches between epochs do not look alike. Doing so will eventually make our model more robust.\n\n- `num_workers`, denotes the number of processes that generate batches in parallel. If you have more cores in your CPU, you can set it to number of cores in your CPU. Since, Kaggle provides a 2 core CPU, I have set it to 2\n","metadata":{}},{"cell_type":"code","source":"# helper function to show a batch of training instances\ndef show_batch(data):\n    for images, labels in data:\n        fig, ax = plt.subplots(figsize=(30, 30))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=8).permute(1, 2, 0))\n        break","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:31:59.631437Z","iopub.execute_input":"2022-11-22T10:31:59.631806Z","iopub.status.idle":"2022-11-22T10:31:59.639228Z","shell.execute_reply.started":"2022-11-22T10:31:59.631732Z","shell.execute_reply":"2022-11-22T10:31:59.638369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Images for first batch of training\nshow_batch(train_dl) ","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:31:59.640746Z","iopub.execute_input":"2022-11-22T10:31:59.641114Z","iopub.status.idle":"2022-11-22T10:32:06.117725Z","shell.execute_reply.started":"2022-11-22T10:31:59.641076Z","shell.execute_reply":"2022-11-22T10:32:06.116552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling ","metadata":{}},{"cell_type":"markdown","source":"When working with image datasets, it is recommended to utilize GPUs instead of CPUs. This is because CPUs are designed for general-purpose tasks, while GPUs are specifically optimized for training deep learning models. GPUs excel at processing multiple computations simultaneously, thanks to their numerous cores. This parallel processing capability makes them well-suited for the complex computations required in deep learning. Moreover, deep learning often involves handling large amounts of data, and GPUs offer high memory bandwidth, making them particularly suitable for these computationally intensive tasks.\nTo seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required","metadata":{}},{"cell_type":"markdown","source":"#### Some helper functions","metadata":{}},{"cell_type":"code","source":"# for moving data into GPU (if available)\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available:\n        return torch.device(\"cuda\")\n    else:\n        return torch.device(\"cpu\")\n\n# for moving data to device (CPU or GPU)\ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\n# for loading in the device (GPU if available else CPU)\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl:\n            yield to_device(b, self.device)\n        \n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:32:06.119147Z","iopub.execute_input":"2022-11-22T10:32:06.119596Z","iopub.status.idle":"2022-11-22T10:32:06.130394Z","shell.execute_reply.started":"2022-11-22T10:32:06.119559Z","shell.execute_reply":"2022-11-22T10:32:06.129355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the device we are working with","metadata":{}},{"cell_type":"code","source":"device = get_default_device()\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:32:06.131729Z","iopub.execute_input":"2022-11-22T10:32:06.132314Z","iopub.status.idle":"2022-11-22T10:32:06.149712Z","shell.execute_reply.started":"2022-11-22T10:32:06.132277Z","shell.execute_reply":"2022-11-22T10:32:06.145513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wrap up our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available)","metadata":{}},{"cell_type":"code","source":"# Moving data into GPU\ntrain_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:32:06.151071Z","iopub.execute_input":"2022-11-22T10:32:06.151688Z","iopub.status.idle":"2022-11-22T10:32:06.156067Z","shell.execute_reply.started":"2022-11-22T10:32:06.151653Z","shell.execute_reply":"2022-11-22T10:32:06.155389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the model architecture ","metadata":{}},{"cell_type":"markdown","source":"*We are going to use **ResNet**, which have been one of the major breakthrough in computer vision since they were introduced in 2015.*","metadata":{}},{"cell_type":"markdown","source":"If you want to learn more about ResNets read the following articles:\n- [Understanding and Visualizing ResNets](https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8#:~:text=ResNet%20Layers,layers%20remains%20the%20same%20%E2%80%94%204.)\n- [Overview of ResNet and its variants](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)\n- [Paper with code implementation](https://paperswithcode.com/method/resnet)","metadata":{}},{"cell_type":"markdown","source":"In ResNets, there is a departure from the conventional neural network architecture. Instead of each layer only feeding into the next layer, we incorporate residual blocks. These blocks allow each layer to not only contribute to the subsequent layer but also directly connect to layers that are about 2-3 hops away. This design is implemented to address the issue of overfitting, which occurs when the validation loss stops decreasing and starts to increase while the training loss continues to decrease. By introducing these residual connections, the network can mitigate the risk of overfitting and improve overall performance. This also helps in preventing [vanishing gradient problem](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484) and allow us to train deep neural networks. Here is a simple residual block:","metadata":{}},{"cell_type":"markdown","source":"#### Residual Block code implementation","metadata":{}},{"cell_type":"code","source":"class SimpleResidualBlock(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n        self.relu2 = nn.ReLU()\n        \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.relu1(out)\n        out = self.conv2(out)\n        return self.relu2(out) + x # ReLU can be applied before or after adding the input","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:32:06.157486Z","iopub.execute_input":"2022-11-22T10:32:06.158046Z","iopub.status.idle":"2022-11-22T10:32:06.167394Z","shell.execute_reply.started":"2022-11-22T10:32:06.158012Z","shell.execute_reply":"2022-11-22T10:32:06.166315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Then we define our `ImageClassificationBase` class whose functions are:**\n\n- `training_step` - To figure out how “wrong” the model is going after training or validation step.We are using this function other than just an accuracy metric that is likely not going to be differentiable (this would mean that the gradient can’t be determined, which is necessary for the model to improve during training)\n\nA quick look at the PyTorch docs that yields the cost function: [cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#cross-entropy).\n\n- `validation_step` - Because an accuracy metric can’t be used while training the model, doesn’t mean it shouldn’t be implemented! Accuracy in this case would be measured by a threshold, and counted if the difference between the model’s prediction and the actual label is lower than that threshold.\n- `validation_epoch_end` - We want to track the validation losses/accuracies and train losses after each epoch, and every time we do so we have to make sure the gradient is not being tracked.\n- `epoch_end` - We also want to print validation losses/accuracies, train losses and learning rate too because we are using learning rate scheduler (which will change the learning rate after every batch of training) after each epoch.\n\nWe also define an `accuracy` function which calculates the overall accuracy of the model on an entire batch of outputs, so that we can use it as a metric in `fit_one_cycle`","metadata":{}},{"cell_type":"code","source":"# for calculating the accuracy\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\n\n# base class for the model\nclass ImageClassificationBase(nn.Module):\n    \n    def training_step(self, batch):\n        images, labels = batch\n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch\n        out = self(images)                   # Generate prediction\n        loss = F.cross_entropy(out, labels)  # Calculate loss\n        acc = accuracy(out, labels)          # Calculate accuracy\n        return {\"val_loss\": loss.detach(), \"val_accuracy\": acc}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x[\"val_loss\"] for x in outputs]\n        batch_accuracy = [x[\"val_accuracy\"] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()       # Combine loss  \n        epoch_accuracy = torch.stack(batch_accuracy).mean()\n        return {\"val_loss\": epoch_loss, \"val_accuracy\": epoch_accuracy} # Combine accuracies\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_accuracy']))\n        ","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:32:06.168840Z","iopub.execute_input":"2022-11-22T10:32:06.169149Z","iopub.status.idle":"2022-11-22T10:32:06.183920Z","shell.execute_reply.started":"2022-11-22T10:32:06.169109Z","shell.execute_reply":"2022-11-22T10:32:06.183074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the final architecture of our model ","metadata":{}},{"cell_type":"code","source":"# Architecture for training\n\n# convolution block with BatchNormalization\ndef ConvBlock(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n             nn.BatchNorm2d(out_channels),\n             nn.ReLU(inplace=True)]\n    if pool:\n        layers.append(nn.MaxPool2d(4))\n    return nn.Sequential(*layers)\n\n\n# resnet architecture \nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_diseases):\n        super().__init__()\n        \n        self.conv1 = ConvBlock(in_channels, 64)\n        self.conv2 = ConvBlock(64, 128, pool=True) # out_dim : 128 x 64 x 64 \n        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))\n        \n        self.conv3 = ConvBlock(128, 256, pool=True) # out_dim : 256 x 16 x 16\n        self.conv4 = ConvBlock(256, 512, pool=True) # out_dim : 512 x 4 x 44\n        self.res2 = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))\n        \n        self.classifier = nn.Sequential(nn.MaxPool2d(4),\n                                       nn.Flatten(),\n                                       nn.Linear(512, num_diseases))\n        \n    def forward(self, xb): # xb is the loaded batch\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.classifier(out)\n        return out        ","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:32:06.185218Z","iopub.execute_input":"2022-11-22T10:32:06.185838Z","iopub.status.idle":"2022-11-22T10:32:06.200469Z","shell.execute_reply.started":"2022-11-22T10:32:06.185804Z","shell.execute_reply":"2022-11-22T10:32:06.199278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we define a model object and transfer it into the device with which we are working...","metadata":{}},{"cell_type":"code","source":"# defining the model and moving it to the GPU\nmodel = to_device(ResNet9(3, len(train.classes)), device) \nmodel","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:32:06.202133Z","iopub.execute_input":"2022-11-22T10:32:06.202559Z","iopub.status.idle":"2022-11-22T10:32:06.284471Z","shell.execute_reply.started":"2022-11-22T10:32:06.202516Z","shell.execute_reply":"2022-11-22T10:32:06.283698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Getting a nicely formatted summary of our model (like in Keras). Pytorch doesn't support it natively. So, we need to install the `torchsummary` library (discussed earlier)*","metadata":{}},{"cell_type":"code","source":"# getting summary of the model\nINPUT_SHAPE = (3, 256, 256)\nprint(summary(model.cuda(), (INPUT_SHAPE)))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:32:06.289744Z","iopub.execute_input":"2022-11-22T10:32:06.289999Z","iopub.status.idle":"2022-11-22T10:32:06.942586Z","shell.execute_reply.started":"2022-11-22T10:32:06.289974Z","shell.execute_reply":"2022-11-22T10:32:06.941831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"markdown","source":"Before we train the model, Let’s define a utility functionan `evaluate` function, which will perform the validation phase, and a `fit_one_cycle` function which will perform the entire training process. In `fit_one_cycle`, we have use some techniques:","metadata":{}},{"cell_type":"markdown","source":"- **Learning Rate Scheduling**: Instead of utilizing a static learning rate, we will implement a learning rate scheduler that adjusts the learning rate after each training batch. There are several strategies available for modifying the learning rate during training, and the one we will employ is known as the \"One Cycle Learning Rate Policy\". This approach involves commencing with a low learning rate and progressively increasing it batch by batch until reaching a higher learning rate for approximately 30% of the epochs. Subsequently, the learning rate is gradually decreased to a very low value for the remaining epochs. By employing this policy, we can optimize the learning rate throughout the training process and potentially improve the model's performance\n\n- **Weight Decay**: In addition, we employ weight decay as a regularization technique to prevent the weights from growing excessively large. This is achieved by incorporating an extra term into the loss function. By using weight decay, we can effectively control the magnitude of the weights and potentially improve the model's generalization capabilities. \n\n- **Gradient Clipping**: In addition to managing the weights and outputs of the layers, it is beneficial to constrain the values of gradients within a specific range. This helps prevent any undesirable parameter updates caused by excessively large gradient values. This technique, known as gradient clipping, is a straightforward yet powerful approach to ensure stable and controlled training of the model.\n\nWe'll also record the learning rate used for each batch.","metadata":{}},{"cell_type":"code","source":"# for training\n@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n    \n\ndef fit_OneCycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0,\n                grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # scheduler for one cycle learniing rate\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))\n    \n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n                \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # recording and updating learning rates\n            lrs.append(get_lr(optimizer))\n            sched.step()\n            \n    \n        # validation\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n        \n    return history\n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:32:06.946093Z","iopub.execute_input":"2022-11-22T10:32:06.946380Z","iopub.status.idle":"2022-11-22T10:32:06.958789Z","shell.execute_reply.started":"2022-11-22T10:32:06.946345Z","shell.execute_reply":"2022-11-22T10:32:06.956715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check our validation loss and accuracy","metadata":{}},{"cell_type":"code","source":"%%time\nhistory = [evaluate(model, valid_dl)]\nhistory","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:32:06.962082Z","iopub.execute_input":"2022-11-22T10:32:06.962360Z","iopub.status.idle":"2022-11-22T10:33:37.880605Z","shell.execute_reply.started":"2022-11-22T10:32:06.962329Z","shell.execute_reply":"2022-11-22T10:33:37.879706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there are randomly initialized weights, that is why accuracy come to near 0.019 (that is 1.9% chance of getting the right answer or you can say model randomly chooses a class).\nNow, declare some hyper parameters for the training of the model. We can change it if result is not satisfactory.","metadata":{}},{"cell_type":"code","source":"epochs = 2\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:33:37.882076Z","iopub.execute_input":"2022-11-22T10:33:37.882403Z","iopub.status.idle":"2022-11-22T10:33:37.887703Z","shell.execute_reply.started":"2022-11-22T10:33:37.882373Z","shell.execute_reply":"2022-11-22T10:33:37.886883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's start training our model ....\n\nNote: The following cell may take 15 mins to 45 mins to run depending on your GPU. In kaggle (P100 GPU) it took around 20 mins of Wall Time.","metadata":{}},{"cell_type":"code","source":"%%time\nhistory += fit_OneCycle(epochs, max_lr, model, train_dl, valid_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=1e-4, \n                             opt_func=opt_func)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:33:37.889157Z","iopub.execute_input":"2022-11-22T10:33:37.889740Z","iopub.status.idle":"2022-11-22T10:53:28.347994Z","shell.execute_reply.started":"2022-11-22T10:33:37.889697Z","shell.execute_reply":"2022-11-22T10:53:28.346170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We got an accuracy of 99.2 % ","metadata":{}},{"cell_type":"markdown","source":"# Plotting ","metadata":{}},{"cell_type":"markdown","source":"#### Helper functions for plotting","metadata":{}},{"cell_type":"code","source":"def plot_accuracies(history):\n    accuracies = [x['val_accuracy'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n\ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');\n    \ndef plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:53:28.376557Z","iopub.execute_input":"2022-11-22T10:53:28.377122Z","iopub.status.idle":"2022-11-22T10:53:28.386769Z","shell.execute_reply.started":"2022-11-22T10:53:28.377073Z","shell.execute_reply":"2022-11-22T10:53:28.386182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation Accuracy","metadata":{}},{"cell_type":"code","source":"plot_accuracies(history)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:53:28.387984Z","iopub.execute_input":"2022-11-22T10:53:28.388497Z","iopub.status.idle":"2022-11-22T10:53:28.527069Z","shell.execute_reply.started":"2022-11-22T10:53:28.388461Z","shell.execute_reply":"2022-11-22T10:53:28.526115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation loss","metadata":{}},{"cell_type":"code","source":"plot_losses(history)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:53:28.528323Z","iopub.execute_input":"2022-11-22T10:53:28.528658Z","iopub.status.idle":"2022-11-22T10:53:28.690588Z","shell.execute_reply.started":"2022-11-22T10:53:28.528621Z","shell.execute_reply":"2022-11-22T10:53:28.689680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning Rate overtime","metadata":{}},{"cell_type":"code","source":"plot_lrs(history)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:53:28.692388Z","iopub.execute_input":"2022-11-22T10:53:28.692767Z","iopub.status.idle":"2022-11-22T10:53:28.812505Z","shell.execute_reply.started":"2022-11-22T10:53:28.692727Z","shell.execute_reply":"2022-11-22T10:53:28.811640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing model on test data","metadata":{}},{"cell_type":"markdown","source":"**We only have 33 images in test data, so let's check the model on all images**","metadata":{}},{"cell_type":"code","source":"test_dir = \"../input/new-plant-diseases-dataset/test\"\ntest = ImageFolder(test_dir, transform=transforms.ToTensor())","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:53:28.814056Z","iopub.execute_input":"2022-11-22T10:53:28.814536Z","iopub.status.idle":"2022-11-22T10:53:28.842003Z","shell.execute_reply.started":"2022-11-22T10:53:28.814499Z","shell.execute_reply":"2022-11-22T10:53:28.841393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = sorted(os.listdir(test_dir + '/test')) # since images in test folder are in alphabetical order\ntest_images","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:53:28.843388Z","iopub.execute_input":"2022-11-22T10:53:28.843723Z","iopub.status.idle":"2022-11-22T10:53:28.850701Z","shell.execute_reply.started":"2022-11-22T10:53:28.843688Z","shell.execute_reply":"2022-11-22T10:53:28.849818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_image(img, model):\n    \"\"\"Converts image to array and return the predicted class\n        with highest probability\"\"\"\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n\n    return train.classes[preds[0].item()]","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:53:28.851849Z","iopub.execute_input":"2022-11-22T10:53:28.852453Z","iopub.status.idle":"2022-11-22T10:53:28.859044Z","shell.execute_reply.started":"2022-11-22T10:53:28.852414Z","shell.execute_reply":"2022-11-22T10:53:28.858212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting first image\nimg, label = test[0]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_images[0], ', Predicted:', predict_image(img, model))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:53:28.860239Z","iopub.execute_input":"2022-11-22T10:53:28.860569Z","iopub.status.idle":"2022-11-22T10:53:29.013263Z","shell.execute_reply.started":"2022-11-22T10:53:28.860532Z","shell.execute_reply":"2022-11-22T10:53:29.012445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting all predictions (actual label vs predicted)\nfor i, (img, label) in enumerate(test):\n    print('Label:', test_images[i], ', Predicted:', predict_image(img, model))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:53:29.014696Z","iopub.execute_input":"2022-11-22T10:53:29.015251Z","iopub.status.idle":"2022-11-22T10:53:29.476350Z","shell.execute_reply.started":"2022-11-22T10:53:29.015212Z","shell.execute_reply":"2022-11-22T10:53:29.475291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can see that the model predicted all the test images perfectly!!!!**","metadata":{}},{"cell_type":"markdown","source":"# Saving the model","metadata":{}},{"cell_type":"markdown","source":"**There are several ways to save the model in Pytorch, following are the two most common ways**","metadata":{}},{"cell_type":"markdown","source":"1. **Save/Load `state_dict` (Recommended)**\n\nWhen saving a model for inference, it is only necessary to save the trained model’s learned parameters. Saving the model’s `state_dict` with the `torch.save()` function will give you the most flexibility for restoring the model later, which is why it is the recommended method for saving models.\n\nA common PyTorch convention is to save models using either a `.pt` or `.pth` file extension.\n\nRemember that you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.","metadata":{}},{"cell_type":"code","source":"# saving to the kaggle working directory\nPATH = './plant-disease-model.pth'  \ntorch.save(model.state_dict(), PATH)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:53:29.477855Z","iopub.execute_input":"2022-11-22T10:53:29.478208Z","iopub.status.idle":"2022-11-22T10:53:29.550938Z","shell.execute_reply.started":"2022-11-22T10:53:29.478155Z","shell.execute_reply":"2022-11-22T10:53:29.549957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. **Save/Load Entire Model**\n\nThis save/load process uses the most intuitive syntax and involves the least amount of code. Saving a model in this way will save the entire module using Python’s [pickle](https://docs.python.org/3/library/pickle.html) module. One drawback of this approach is the tight coupling between the serialized data and the specific classes and directory structure used during model saving. This limitation arises because the pickle module does not directly save the model class itself. Instead, it saves a reference to the file containing the class, which is then utilized during the loading process. Consequently, when using the serialized model in different projects or after making changes to the code structure, there is a potential for compatibility issues and unexpected errors to occur. This lack of flexibility can lead to code breakage and requires careful consideration when implementing the serialized model in different contexts.","metadata":{}},{"cell_type":"code","source":"# saving the entire model to working directory\nPATH = './plant-disease-model-complete.pth'\ntorch.save(model, PATH)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T10:53:29.553090Z","iopub.execute_input":"2022-11-22T10:53:29.553619Z","iopub.status.idle":"2022-11-22T10:53:29.610571Z","shell.execute_reply.started":"2022-11-22T10:53:29.553578Z","shell.execute_reply":"2022-11-22T10:53:29.609621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"Residual Neural Networks (ResNets) have shown remarkable performance in image classification tasks, particularly when specific parameters are adjusted and techniques such as learning rate scheduling, gradient clipping, and weight decay are implemented. By fine-tuning these parameters and employing these techniques, ResNets can achieve superior accuracy and robustness in classifying images. These optimizations contribute to the overall success of ResNets in image classification tasks, making them a highly effective choice for such applications. The model is able to predict every image in test set perfectly without any errors !!!!","metadata":{}},{"cell_type":"markdown","source":"# References\n- [CIFAR10 ResNet Implementation](https://jovian.ai/aakashns/05b-cifar10-resnet)\n- [PyTorch docs](https://pytorch.org/)","metadata":{}}]}